{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Word2Vec_Embedding+CNN] Sentiment Analysis.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Nfy3FFf8qxv6",
        "GEMfFuq-i8Pr",
        "mSet1q5vPZnM"
      ],
      "authorship_tag": "ABX9TyM9OOvm1q6KIp2jI6hQB2kP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setthawut8/ai/blob/main/%5BWord2Vec_Embedding%2BCNN%5D_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inspiration\n",
        "https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
        "\n",
        "=================================================================\n",
        "https://pythainlp.github.io/tutorials/notebooks/word2vec_examples.html\n",
        "\n",
        "+\n",
        "\n",
        "https://ml2021.medium.com/multi-class-text-classification-using-cnn-and-word2vec-b17daff45260\n",
        "\n",
        "(https://github.com/ML2021/TF-RNN-CNN-text-processing/blob/main/Multi%20Class%20Text%20Classification%20word2vec%20using%20CNN.ipynb)"
      ],
      "metadata": {
        "id": "_rsLQBqFqXG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Download and Installation"
      ],
      "metadata": {
        "id": "Nfy3FFf8qxv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4LYmUQbbp4yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2f9200-251e-416e-963b-a4f28a7d7f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-04 03:42:11--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/octet-stream]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-07-04 03:42:11 (64.9 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf '/content/review_polarity.tar.gz'"
      ],
      "metadata": {
        "id": "DUStd7ZXq2pe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "metadata": {
        "id": "eHSZvsyAqlc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ef34e7-2368-47d5-e6c7-3179eb854a9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"neg: \",len(os.listdir('/content/txt_sentoken/neg')), \" pos: \", len(os.listdir('/content/txt_sentoken/pos')))"
      ],
      "metadata": {
        "id": "K1TtGEh1tG0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5294e2-ec64-4805-f503-516f281abe01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg:  1000  pos:  1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Defind a Vocabulary"
      ],
      "metadata": {
        "id": "GEMfFuq-i8Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        " \n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        " \n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab, True)\n",
        "process_docs('txt_sentoken/pos', vocab, True)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "UCEVIUwxO7ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b71280-6f17-401f-fd8e-b7c62d69fb86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        " \n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "metadata": {
        "id": "H9SH7sqZOy1V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Loading Doc & Vocab"
      ],
      "metadata": {
        "id": "mSet1q5vPZnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        " \n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents"
      ],
      "metadata": {
        "id": "rf_QxGfiPd4-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        " \n",
        "# load all training reviews\n",
        "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        " \n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        " \n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
        "negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        " \n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "Ql-SrKWrQ-7j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Train Word2Vec Model"
      ],
      "metadata": {
        "id": "Ga-8OIuuPq_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "\tclean_lines = list()\n",
        "\tlines = doc.splitlines()\n",
        "\tfor line in lines:\n",
        "\t\t# split into tokens by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# remove punctuation from each token\n",
        "\t\ttable = str.maketrans('', '', punctuation)\n",
        "\t\ttokens = [w.translate(table) for w in tokens]\n",
        "\t\t# filter out tokens not in vocab\n",
        "\t\ttokens = [w for w in tokens if w in vocab]\n",
        "\t\tclean_lines.append(tokens)\n",
        "\treturn clean_lines\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
        "\t\t# add lines to list\n",
        "\t\tlines += doc_lines\n",
        "\treturn lines\n",
        "\n",
        "  \n",
        "# load training data\n",
        "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
        "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
        "sentences = negative_docs + positive_docs\n",
        "print('Total training sentences: %d' % len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI5GbWxlSVYg",
        "outputId": "bbe53593-479c-48be-a715-de61df46be49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training sentences: 58109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train word2vec model\n",
        "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
        "# summarize vocabulary size in model\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "# save model in ASCII (word2vec) format\n",
        "filename = 'embedding_word2vec.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "metadata": {
        "id": "TJvyxoJrPtdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58e808a-be0c-467f-ddbc-d36213569b18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) Load Embedding and Train the Model with Testing"
      ],
      "metadata": {
        "id": "nRN7wJ_SRQVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        " \n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix"
      ],
      "metadata": {
        "id": "44brkyVLTIaB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding from file\n",
        "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=True)\n",
        " \n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "id": "N4w_QoruRMfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5cdb6e-6139-4384-8ac1-f64b6ca1827e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1317, 100)         2576800   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1313, 128)         64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 656, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 83968)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 83969     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,724,897\n",
            "Trainable params: 2,724,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "57/57 - 13s - loss: 0.7403 - accuracy: 0.5289 - 13s/epoch - 235ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 1s - loss: 0.6697 - accuracy: 0.5922 - 520ms/epoch - 9ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 1s - loss: 0.5901 - accuracy: 0.7122 - 530ms/epoch - 9ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 1s - loss: 0.4073 - accuracy: 0.8472 - 524ms/epoch - 9ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 1s - loss: 0.1991 - accuracy: 0.9578 - 522ms/epoch - 9ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 1s - loss: 0.0702 - accuracy: 0.9983 - 522ms/epoch - 9ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 1s - loss: 0.0250 - accuracy: 1.0000 - 520ms/epoch - 9ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 1s - loss: 0.0111 - accuracy: 1.0000 - 527ms/epoch - 9ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 1s - loss: 0.0062 - accuracy: 1.0000 - 516ms/epoch - 9ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 1s - loss: 0.0041 - accuracy: 1.0000 - 522ms/epoch - 9ms/step\n",
            "Test Accuracy: 68.000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Accuracy\n",
        "# Not Trainable Embedding Layer: 0.55 \n",
        "# Trainable Embedding Layer: 0.68"
      ],
      "metadata": {
        "id": "xLJShjWTRpBP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y0p3Er5zShi_"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}
