{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[EN2THDict4Vader] Sentiment Analysis.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "soHPHarYfPiE",
        "w6SC8-O9n1QC",
        "p8Fac0yrfSom",
        "MMEssEfRfVvZ",
        "9gcjNMssuN-t",
        "VCizWC0Prw8n"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setthawut8/ai/blob/main/modified/%5BEN2THDict4Vader%5D_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspiration: https://github.com/cjhutto/vaderSentiment"
      ],
      "metadata": {
        "id": "Xwfp0if_mtbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "soHPHarYfPiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "unn-Zv9P8e4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cea9bf1f-3c16-41c4-9778-d85490ea5de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.30)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.64.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.12.0+cu113)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 92.9 MB/s \n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 86.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2022.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.21.6)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (5.8.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 81.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq) (4.1.1)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq) (3.8.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=b30bfc04c26a53b4c2182c1b78778d0859cbc4a834ab123d6fa4c70998814e6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.5.1 colorama-0.4.5 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.5.1 sacrebleu-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.0.8-py3-none-any.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 7.6 MB/s \n",
            "\u001b[?25hCollecting tinydb>=3.0\n",
            "  Downloading tinydb-4.7.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (4.1.1)\n",
            "Installing collected packages: tinydb, pythainlp\n",
            "Successfully installed pythainlp-3.0.8 tinydb-4.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=efa321cf59520ad490b11d6de998fb142bf0dea006fb05b242b418ff12bff5ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "vaderSentiment"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#For NLP PyThaiNLP translation\n",
        "!pip install fairseq\n",
        "!pip install pythainlp\n",
        "!pip install sacremoses\n",
        "!pip install transformers\n",
        "\n",
        "#For Vader Sentiment\n",
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1T0-WVLjBDW",
        "outputId": "2ea33cb8-6153-4841-8a36-c59a677306d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download original Vader\n",
        "#!git clone https://github.com/cjhutto/vaderSentiment.git"
      ],
      "metadata": {
        "id": "TJSAtlrKMY6g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Methods for the Vader Modification"
      ],
      "metadata": {
        "id": "w6SC8-O9n1QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the vader lexicon\n",
        "#https://stackoverflow.com/questions/45275166/is-vader-sentimentintensityanalyzer-multilingual#:~:text=Change%20the-,vader_lexicon,-.txt\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "6y-VeeXb8owT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to add or delete new word(s) in the Vader Lexicon as the link below:\n",
        "#https://stackoverflow.com/questions/40481348/is-it-possible-to-edit-nltks-vader-sentiment-lexicon#:~:text=in%20this%20post%3A-,from%20nltk.sentiment.vader%20import%20SentimentIntensityAnalyzer,-new_words%20%3D%20%7B%0A%20%20%20%20%27foo\n",
        "\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# new_words = {\n",
        "#     'foo': 2.0,\n",
        "#     'bar': -3.4,\n",
        "# }\n",
        "\n",
        "##Add some words\n",
        "# SIA = SentimentIntensityAnalyzer()\n",
        "# SIA.lexicon.update(new_words)\n",
        "\n",
        "## Delete some words\n",
        "# SIA = SentimentIntensityAnalyzer()\n",
        "# SIA.lexicon.pop('no')"
      ],
      "metadata": {
        "id": "PPYxw8tD9AQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add New Words to Vader"
      ],
      "metadata": {
        "id": "CA0rkpkpakH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1KStSddVTk_VlkD--UxQbAQgNR56ZlpAT\n",
        "  \n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6iFtDOWXHLG",
        "outputId": "e6f6692e-668f-4300-a300-de21e0ba375f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KStSddVTk_VlkD--UxQbAQgNR56ZlpAT\n",
            "To: /content/th_vader_lexicon.txt\n",
            "\r  0% 0.00/193k [00:00<?, ?B/s]\r100% 193k/193k [00:00<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to add or delete new word(s) in the Vader Lexicon as the link below:\n",
        "# https://stackoverflow.com/questions/40481348/is-it-possible-to-edit-nltks-vader-sentiment-lexicon#:~:text=in%20this%20post%3A-,from%20nltk.sentiment.vader%20import%20SentimentIntensityAnalyzer,-new_words%20%3D%20%7B%0A%20%20%20%20%27foo\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def open_txt(txt):\n",
        "  file = open(txt,'rt')\n",
        "  th_lexicon = file.read()\n",
        "  file.close()\n",
        "  return th_lexicon"
      ],
      "metadata": {
        "id": "Rf2aoaT4bITR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open translated lexicon from the EN Vader lexicon\n",
        "th_data = open_txt('/content/th_vader_lexicon.txt')\n",
        "\n",
        "#Create arrays for the data\n",
        "th_data_lists = th_data.split('\\n')\n",
        "th_data_list_lists = [x.split('\\t') for x in th_data_lists]\n",
        "\n",
        "#Change lists into dicts to save in the Vader lexicon\n",
        "new_th_data_dicts = {k:v for k, v, w, p in th_data_list_lists}\n",
        "\n",
        "#Save new words in the lexicon\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "SIA.lexicon.update(new_th_data_dicts)"
      ],
      "metadata": {
        "id": "wfPKUWnEbXSF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thai_words = 'น่าชื่นชม'\n",
        "SIA.polarity_scores(thai_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "MA2QWI8JdelF",
        "outputId": "0fd8f57f-f321-4da8-f7fc-5e8ea18be5b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-81d159ce8d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthai_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'น่าชื่นชม'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSIA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthai_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_but_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentiment_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentitext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mscore_valence\u001b[0;34m(self, sentiments, text)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0msum_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0;31m# compute and add emphasis from punctuation in text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mpunct_emph_amplifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_punctuation_emphasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vader EN Text"
      ],
      "metadata": {
        "id": "p8Fac0yrfSom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
        "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
        "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
        "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
        "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
        "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
        "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
        "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
        "\n",
        "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
        "B_INCR = 0.293\n",
        "B_DECR = -0.293\n",
        "\n",
        "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
        "C_INCR = 0.733\n",
        "N_SCALAR = -0.74\n",
        "\n",
        "BOOSTER_DICT = {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR,\n",
        "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
        "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
        "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR,\n",
        "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
        "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
        "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR,\n",
        "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
        "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR,\n",
        "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR,\n",
        "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
        "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
        "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
        "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
        "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
        "     \"very\": B_INCR,\n",
        "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
        "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
        "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
        "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
        "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
        "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
        "\n",
        "# check for sentiment laden idioms that do not contain lexicon words (future work, not yet implemented)\n",
        "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
        "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
        "                          \"upper hand\": 1, \"break a leg\": 2,\n",
        "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
        "                          \"on the ball\": 2, \"under the weather\": -2}\n",
        "\n",
        "# check for special case idioms and phrases containing lexicon words\n",
        "SPECIAL_CASES = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5, \"bus stop\": 0.0,\n",
        "                 \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3,\n",
        "                 \"beating heart\": 3.1, \"broken heart\": -2.9 }\n"
      ],
      "metadata": {
        "id": "AMwkdNuXAuSF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EN2TH Translation"
      ],
      "metadata": {
        "id": "MMEssEfRfVvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Translation\n",
        "from pythainlp.translate import Translate\n",
        "en2th = Translate('en', 'th')"
      ],
      "metadata": {
        "id": "xacELB0e_kaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c485fb74-482d-44cb-d2a1-bb7d4a786d06"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-18 04:01:11 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: scb_1m_en-th_moses\n",
            "- Downloading: scb_1m_en-th_moses 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1174648148/1174648148 [01:05<00:00, 17802889.13it/s]\n",
            "2022-07-18 04:02:32 | INFO | fairseq.file_utils | loading archive file /root/pythainlp-data/scb_1m_en-th_moses/SCB_1M-MT_OPUS+TBASE_en-th_moses-spm_130000-16000_v1.0/models\n",
            "2022-07-18 04:02:32 | INFO | fairseq.file_utils | loading archive file /root/pythainlp-data/scb_1m_en-th_moses/SCB_1M-MT_OPUS+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab\n",
            "2022-07-18 04:02:36 | INFO | fairseq.tasks.translation | [en] dictionary: 130000 types\n",
            "2022-07-18 04:02:36 | INFO | fairseq.tasks.translation | [th] dictionary: 15984 types\n",
            "2022-07-18 04:02:38 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/1m-scb+mt-opus_27.5.2020/en-th/moses-spm/130000-16000/log', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 9750, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 9750, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 75, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/1m-scb+mt-opus_27.5.2020/en-th/moses-spm/130000-16000', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 25, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/root/pythainlp-data/scb_1m_en-th_moses/SCB_1M-MT_OPUS+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=25, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=75, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=9750, max_tokens_valid=9750, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', patience=-1, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/1m-scb+mt-opus_27.5.2020/en-th/moses-spm/130000-16000', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_min_lr=-1, target_lang='th', task='translation', tensorboard_logdir='./checkpoints/1m-scb+mt-opus_27.5.2020/en-th/moses-spm/130000-16000/log', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[16], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0), 'task': {'_name': 'translation', 'data': '/root/pythainlp-data/scb_1m_en-th_moses/SCB_1M-MT_OPUS+TBASE_en-th_moses-spm_130000-16000_v1.0/vocab', 'source_lang': 'en', 'target_lang': 'th', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': 1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': None, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In The PY File"
      ],
      "metadata": {
        "id": "koerW4nRfeom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATE_translated = [en2th.translate(x) for x in NEGATE]\n",
        "NEGATE_translated_modified = [x for x in NEGATE_translated if x != 'Name=สมุดที่อยู่ - KName']\n",
        "print(NEGATE_translated_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rakHlc7WrsK",
        "outputId": "998cad3b-5d94-4e02-cf78-3364fde43a37"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ไม่ใช่นะ', 'ไม่อ่ะ', 'ไม่สามารถ', 'ไม่ได้อ่ะ', 'ไม่สามารถ', 'ไม่อ่ะ', 'ไม่ได้', 'ไม่ได้', 'ไม่ใช่', 'ไม่ได้', 'ทําไม่ได้', 'ไม่กล้าไม่กล้า', 'ไม่ได้', 'ไม่ได้', 'ไม่อ่ะ', 'ไม่อ่ะ', 'ยังไม่ได้', 'ไม่ใช่', 'ไม่เหมือนกัน', 'อย่านะ', 'ไม่ได้', 'ยังเลย', 'ยังเลย', 'ไม่ใช่', 'ไม่ได้', 'อย่าเลย', 'ไม่จําเป็น', 'ไม่เคย', 'ไม่มี', 'ไม่อ่ะ', 'หรือ', 'ไม่ใช่', 'ไม่มีอะไร', 'ไม่มีที่ไหนเลย', 'ไม่น่าจะ', 'ไม่ใช่', 'ไม่', 'ไม่ควรจะ', 'ชันไม่', 'ไม่ควร', 'ไม่ใช่', 'ไม่ได้', 'โดยไม่ต้อง', 'ไม่อ่ะ', 'ไม่อยากหรอก', 'จะไม่', 'จะไม่', 'ไม่ค่อย', 'ไม่ค่อยได้', 'แม้จะ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOOSTER_DICT_modified = {en2th.translate(k): v for k, v in BOOSTER_DICT.items()}\n",
        "print(BOOSTER_DICT_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1p2EdgGEGhs",
        "outputId": "3e896367-ef98-4d7f-a0e1-6b6f4e58d624"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'แน่นอนครับ': 0.293, 'น่าอัศจรรย์': 0.293, 'น่ากลัวจัง': 0.293, 'สมบูรณ์': 0.293, 'มาก': 0.293, 'เด็ด': 0.293, 'ลึก': 0.293, 'ไหล...': 0.293, 'ใหญ่มาก': 0.293, 'อย่างมหาศาล': 0.293, 'ทั้งหมด': 0.293, 'โดยเฉพาะอย่างยิ่ง': 0.293, 'พิเศษ': 0.293, 'พิเศษสุด': 0.293, 'สุดขั้ว': 0.293, 'สุด ๆ': 0.293, 'สวยจริงๆ': 0.293, 'พลิกภาพ': 0.293, 'flippin': 0.293, 'Name=สมุดที่อยู่ - KName': -0.293, 'การไล่ล่า': 0.293, 'ไอ้งั่ง': 0.293, 'ไอ้บ้าเอ๊ย': 0.293, 'ไอ้บ้าเอ้ย': 0.293, 'เต็มที่': 0.293, 'ร่วมเพศ': 0.293, 'อย่างมาก': 0.293, 'สวัสดี': 0.293, 'อย่างสูง': 0.293, 'เหลือเชื่อ': 0.293, 'ไม่น่าเชื่อเลย': 0.293, 'หลัก@ item: inlistbox': 0.293, 'เฉดเดียวกัน': 0.293, 'มากขึ้น': 0.293, 'มากที่สุด': 0.293, 'ล้วน ๆ': 0.293, 'ค่อนข้าง': -0.293, 'จริงๆเลย': 0.293, 'น่าทึ่งมาก': 0.293, 'ดังนั้น': 0.293, 'อย่างทั่วถึง': 0.293, 'รวมทั้งหมด': 0.293, 'โดยสิ้นเชิง': 0.293, 'อูเบอร์ค่ะ': 0.293, 'ผิดปกติ': 0.293, 'ที่สุด': 0.293, 'อย่างที่สุด': 0.293, 'เกือบแล้ว': -0.293, 'แทบจะไม่': -0.293, 'พอแล้ว': -0.293, 'ประมาณว่า': -0.293, 'ประมาณนั้น': -0.293, 'น้อยกว่า': -0.293, 'เล็กน้อย': -0.293, 'มุม:': -0.293, 'บิต': -0.293, 'เป็นครั้งคราว': -0.293, 'บางครั้ง': -0.293, 'บางส่วน': -0.293, 'ขาดแคลน': -0.293}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SENTIMENT_LADEN_IDIOMS_modified = {en2th.translate(k): v for k, v in SENTIMENT_LADEN_IDIOMS.items()}\n",
        "print(SENTIMENT_LADEN_IDIOMS_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kmEf9v5GZsh",
        "outputId": "9024009b-e48e-4f56-cf81-953508b258bb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ตัดมัสตาร์ด': 2, 'มือต่อปาก': -2, 'กลับมือ': -2, 'ระเบิดควัน': -2, 'มือบน': 1, 'หักขา': 2, 'ปรุงอาหารด้วยแก๊ส': 2, 'แบล็กค่ะ': 2, 'สีแดงจ้ะ': -2, 'บนลูกบอล': 2, 'ภายใต้สภาพอากาศ': -2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_CASES_modified = {en2th.translate(k): v for k, v in SPECIAL_CASES.items()}\n",
        "print(SPECIAL_CASES_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOtljSPOGKZq",
        "outputId": "6eb9c70f-1ce8-4ecb-ca07-f451d0bebe77"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'อึ': 3, 'ระเบิด': 3, 'ตูดไม่ดี': 1.5, 'ตัวแสบ': 1.5, 'ป้ายรถเมล์': 0.0, 'ใช่ ถูกต้อง': -2, 'จูบแห่งความตาย': -1.5, 'ตายเพื่อ': 3, 'หัวใจเต้น': 3.1, 'หัวใจสลาย': -2.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In The TXT File"
      ],
      "metadata": {
        "id": "NfjKyw50fhpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data from the txt\n",
        "def open_txt_file(txt_path):\n",
        "  file = open(txt_path, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "data = open_txt_file('/content/vaderSentiment/vaderSentiment/vader_lexicon.txt')"
      ],
      "metadata": {
        "id": "JYIQFOtpSrlt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x8AFdtfCujOv",
        "outputId": "749d1068-dade-476e-ffdd-0dfbe9e3a222"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$:\\t-1.5\\t0.80623\\t[-1, -1, -1, -1, -3, -1, -3, -1, -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create arrays for the data\n",
        "data_lists = data.split('\\n')\n",
        "data_list_lists = [x.split('\\t') for x in data_lists]"
      ],
      "metadata": {
        "id": "yMFGOQRKTL52"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there are more than 4 for each word\n",
        "count_more_than_four = 0\n",
        "\n",
        "for text in data_list_lists:\n",
        "  if len(text) != 4:\n",
        "    count_more_than_four += 1\n",
        "    print(\"The unequal to four index:\", data_list_lists.index(text))\n",
        "  continue\n",
        "print(\"no. of the unequal to four:\", count_more_than_four)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tmpvPk1UcfT",
        "outputId": "8adb3b15-3510-40e4-b433-f74f02ade6ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unequal to four index: 7519\n",
            "no. of the unequal to four: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there are more than one words\n",
        "count_blank_spaces = 0\n",
        "for text in data_list_lists:\n",
        "  if ' ' in text[0]:\n",
        "    count_blank_spaces += 1\n",
        "    print(\"The blank space index:\", data_list_lists.index(text))\n",
        "  continue\n",
        "print(\"no. of blank spaces:\", count_blank_spaces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCnAd7Y-X5be",
        "outputId": "a92d75f4-8132-4fdf-9248-39538d43b7b6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The blank space index: 5\n",
            "The blank space index: 1260\n",
            "The blank space index: 2865\n",
            "The blank space index: 5908\n",
            "The blank space index: 7525\n",
            "The blank space index: 7527\n",
            "The blank space index: 7947\n",
            "The blank space index: 8394\n",
            "The blank space index: 8598\n",
            "The blank space index: 8744\n",
            "The blank space index: 8770\n",
            "The blank space index: 8796\n",
            "The blank space index: 9379\n",
            "The blank space index: 9538\n",
            "The blank space index: 9708\n",
            "The blank space index: 9792\n",
            "The blank space index: 9925\n",
            "The blank space index: 9942\n",
            "no. of blank spaces: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blank_space_lists = [5, 1260, 2865, 5908]\n",
        "\n",
        "for idx in blank_space_lists:\n",
        "  print(data_list_lists[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orLnqFeoYNrs",
        "outputId": "219852fb-8e9b-4e57-9348-1df0de76a349"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"( '}{' )\", '1.6', '0.66332', '[1, 2, 2, 1, 1, 2, 2, 1, 3, 1]']\n",
            "[\"can't stand\", '-2.0', '0.63246', '[-2, -2, -2, -1, -1, -2, -3, -2, -2, -3]']\n",
            "['fed up', '-1.8', '1.249', '[-2, -2, -3, -4, -2, -2, -1, -2, -1, 1]']\n",
            "['screwed up', '-1.5', '0.67082', '[-2, -2, -2, -1, -1, 0, -2, -1, -2, -2]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Method to check as the word is English.\n",
        "# import nltk\n",
        "# nltk.download('words')\n",
        "# from nltk.corpus import words\n",
        "\n",
        "# samplewords=['apple','a%32','j & quod','rectangle','house','fsdfdsoij','fdfd']\n",
        "# [i for i in samplewords if i in words.words()]"
      ],
      "metadata": {
        "id": "_F9QfFY2YqpP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_lists = [5, 1260, 2865, 5908]\n",
        "\n",
        "for idx in index_lists:\n",
        "  sample_data_in_lists = data_list_lists[idx][0]\n",
        "  print(\"Text :\", sample_data_in_lists)\n",
        "  print(\"Is English: \", sample_data_in_lists in words.words())\n",
        "  print(\"==================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12-MYOvpaDLx",
        "outputId": "81aaacb9-3287-4736-fc12-2463c699535b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text : ( '}{' )\n",
            "Is English:  False\n",
            "==================\n",
            "Text : can't stand\n",
            "Is English:  False\n",
            "==================\n",
            "Text : fed up\n",
            "Is English:  False\n",
            "==================\n",
            "Text : screwed up\n",
            "Is English:  False\n",
            "==================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments = []\n",
        "en_sentiment_index = []\n",
        "count_translated = 0\n",
        "count_all_texts = 0\n",
        "for text in data_list_lists:\n",
        "  count_all_texts += 1\n",
        "  #If english, translate\n",
        "  if text[0] in words.words():\n",
        "    en_sentiment_index.append(data_list_lists.index(text))\n",
        "    th_texts = en2th.translate(text[0])\n",
        "    new_th_sentiments = [th_texts, text[1], text[2], text[3]]\n",
        "    th_text_sentiments.append(new_th_sentiments)\n",
        "    count_translated += 1\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "pxFIZqIYTfYF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete error on translation\n",
        "#Name=สมุดที่อยู่ - KName error in translation in PyThaiNLP\n",
        "#@ item: inlistbox\n",
        "th_sentiment_index_delelted = []\n",
        "for arr in th_text_sentiments:\n",
        "  if 'Name=สมุดที่อยู่' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "\n",
        "  elif '@ item: inlistbox' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "\n",
        "  elif 'Name=สมุดที่อยู่ - KName' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)  \n",
        "\n",
        "  elif '@ info: whatsthis' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "  else:\n",
        "    continue\n",
        "#9558"
      ],
      "metadata": {
        "id": "EGQintjNvnsM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_list_lists), count_all_texts, len(th_text_sentiments), count_translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_SpobA7oCB7",
        "outputId": "9ef3d77e-f929-44ac-f0bc-6eef12772d29"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10170 10170 3043 4001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA_IV6oMoFM2",
        "outputId": "89af4203-d817-455f-e7ee-e2207c3b3f8a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ฟู', '-3.7', '0.45826', '[-3, -4, -4, -3, -3, -4, -4, -4, -4, -4]'],\n",
              " ['เทฮัก', '1.9', '0.7', '[3, 1, 2, 2, 1, 2, 3, 2, 1, 2]'],\n",
              " ['มือ', '2.2', '0.87178', '[2, 2, 1, 3, 2, 3, 4, 1, 2, 2]'],\n",
              " ['หัวใจ', '3.2', '0.63246', '[3, 3, 4, 3, 4, 2, 3, 4, 3, 3]'],\n",
              " ['หัวใจ', '3.3', '0.48305', '[4, 3, 3, 3, 4, 3, 3, 3, 4, 3]']]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_en_sentiment = en_sentiment_index[:5]\n",
        "for idx in check_en_sentiment:\n",
        "  print(data_list_lists[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6dCvviKr5bS",
        "outputId": "302188b2-5a46-4fe0-a835-912188395631"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fu', '-3.7', '0.45826', '[-3, -4, -4, -3, -3, -4, -4, -4, -4, -4]']\n",
            "['hak', '1.9', '0.7', '[3, 1, 2, 2, 1, 2, 3, 2, 1, 2]']\n",
            "['hand', '2.2', '0.87178', '[2, 2, 1, 3, 2, 3, 4, 1, 2, 2]']\n",
            "['heart', '3.2', '0.63246', '[3, 3, 4, 3, 4, 2, 3, 4, 3, 3]']\n",
            "['hearts', '3.3', '0.48305', '[4, 3, 3, 3, 4, 3, 3, 3, 4, 3]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Change Arrays into String"
      ],
      "metadata": {
        "id": "9gcjNMssuN-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments[0]"
      ],
      "metadata": {
        "id": "nGWac3R2uyv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\\t for one blank space, \\n for the new spaces\n",
        "join_with_blank_spaces = ['\\t'.join(text) for text in th_text_sentiments]\n",
        "final_th_text_with_sentiment = '\\n'.join(join_with_blank_spaces)"
      ],
      "metadata": {
        "id": "oOceqa3iudbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_th_text_with_sentiment[:50]"
      ],
      "metadata": {
        "id": "HCVH5trEvPrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add translated to the txt file"
      ],
      "metadata": {
        "id": "VCizWC0Prw8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspiration: https://colab.research.google.com/github/computationalcore/introduction-to-python/blob/master/notebooks/4-files/PY0101EN-4-2-WriteFile.ipynb#scrollTo=Ar3kQmbt0uq3\n",
        "\n",
        "lexicon_path = '/content/vaderSentiment/vaderSentiment/vader_lexicon.txt'\n",
        "with open(lexicon_path, 'a') as new_lexicon:\n",
        "  new_lexicon.write(final_th_text_with_sentiment)"
      ],
      "metadata": {
        "id": "MJw8N9v6rziY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r '/content/THvader/content/vaderSentiment/vaderSentiment' '/content/THvader/content'"
      ],
      "metadata": {
        "id": "s1CzourIE6-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/vaderSentiment.zip')"
      ],
      "metadata": {
        "id": "2EiIEv_MFJwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vader Modification\n",
        " (If would like to continue: adjust the py file)"
      ],
      "metadata": {
        "id": "vEIVmygNMRrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #download the latest modified Vader\n",
        "# import gdown\n",
        "# gdown.download_folder('https://drive.google.com/drive/folders/1CsSmn-TvReC_DFWgwoFolOY6qNXhIWGh?usp=sharing', quiet=True)\n",
        "!gdown 19KRMDKlxOI5WOZMboOGF32LAVm_uzgNm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBFgDvRiMgpJ",
        "outputId": "559e4dc4-1c53-49a0-c373-c36966ed5991"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19KRMDKlxOI5WOZMboOGF32LAVm_uzgNm\n",
            "To: /content/vaderSentiment.zip\n",
            "100% 5.25M/5.25M [00:00<00:00, 24.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the modified zip file\n",
        "!unzip '/content/vaderSentiment.zip' -d '/content'\n",
        "\n",
        "## See all codes in the py file in the output cell\n",
        "# !cat /content/vaderSentiment/vaderSentiment/vaderSentiment.py"
      ],
      "metadata": {
        "id": "imowYwy4N9Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create your own pip install"
      ],
      "metadata": {
        "id": "8MfCrBvoWq_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://youtu.be/v4bkJef4W94"
      ],
      "metadata": {
        "id": "AFnO-hfnWsU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}