{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[EN2THDict4Vader] Sentiment Analysis.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setthawut8/ai/blob/main/modified/%5BEN2THDict4Vader%5D_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspiration: https://github.com/cjhutto/vaderSentiment"
      ],
      "metadata": {
        "id": "Xwfp0if_mtbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "soHPHarYfPiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unn-Zv9P8e4i"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq\n",
        "!pip install pythainlp\n",
        "!pip install sacremoses\n",
        "!pip install transformers\n",
        "!pip install vaderSentiment\n",
        "!git clone https://github.com/cjhutto/vaderSentiment.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1T0-WVLjBDW",
        "outputId": "6ab869a3-10aa-4f85-edbb-1343bf5c2fb6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Methods for the Vader Modification"
      ],
      "metadata": {
        "id": "w6SC8-O9n1QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the vader lexicon\n",
        "#https://stackoverflow.com/questions/45275166/is-vader-sentimentintensityanalyzer-multilingual#:~:text=Change%20the-,vader_lexicon,-.txt\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "6y-VeeXb8owT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to add or delete new word(s) in the Vader Lexicon as the link below:\n",
        "#https://stackoverflow.com/questions/40481348/is-it-possible-to-edit-nltks-vader-sentiment-lexicon#:~:text=in%20this%20post%3A-,from%20nltk.sentiment.vader%20import%20SentimentIntensityAnalyzer,-new_words%20%3D%20%7B%0A%20%20%20%20%27foo\n",
        "\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# new_words = {\n",
        "#     'foo': 2.0,\n",
        "#     'bar': -3.4,\n",
        "# }\n",
        "\n",
        "##Add some words\n",
        "# SIA = SentimentIntensityAnalyzer()\n",
        "# SIA.lexicon.update(new_words)\n",
        "\n",
        "## Delete some words\n",
        "# SIA = SentimentIntensityAnalyzer()\n",
        "# SIA.lexicon.pop('no')"
      ],
      "metadata": {
        "id": "PPYxw8tD9AQm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vader EN Text"
      ],
      "metadata": {
        "id": "p8Fac0yrfSom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
        "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
        "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
        "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
        "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
        "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
        "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
        "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
        "\n",
        "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
        "B_INCR = 0.293\n",
        "B_DECR = -0.293\n",
        "\n",
        "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
        "C_INCR = 0.733\n",
        "N_SCALAR = -0.74\n",
        "\n",
        "BOOSTER_DICT = {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR,\n",
        "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
        "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
        "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR,\n",
        "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
        "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
        "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR,\n",
        "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
        "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR,\n",
        "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR,\n",
        "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
        "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
        "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
        "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
        "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
        "     \"very\": B_INCR,\n",
        "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
        "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
        "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
        "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
        "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
        "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
        "\n",
        "# check for sentiment laden idioms that do not contain lexicon words (future work, not yet implemented)\n",
        "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
        "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
        "                          \"upper hand\": 1, \"break a leg\": 2,\n",
        "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
        "                          \"on the ball\": 2, \"under the weather\": -2}\n",
        "\n",
        "# check for special case idioms and phrases containing lexicon words\n",
        "SPECIAL_CASES = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5, \"bus stop\": 0.0,\n",
        "                 \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3,\n",
        "                 \"beating heart\": 3.1, \"broken heart\": -2.9 }\n"
      ],
      "metadata": {
        "id": "AMwkdNuXAuSF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EN2TH Translation"
      ],
      "metadata": {
        "id": "MMEssEfRfVvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Translation\n",
        "from pythainlp.translate import Translate\n",
        "en2th = Translate('en', 'th')"
      ],
      "metadata": {
        "id": "xacELB0e_kaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In The PY File"
      ],
      "metadata": {
        "id": "koerW4nRfeom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATE_translated = [en2th.translate(x) for x in NEGATE]\n",
        "NEGATE_translated_modified = [x for x in NEGATE_translated if x != 'Name=สมุดที่อยู่ - KName']\n",
        "print(NEGATE_translated_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rakHlc7WrsK",
        "outputId": "517be491-41fb-4ff8-b61b-832edb94fa10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ไม่ใช่นะ', 'ไม่อ่ะ', 'ไม่สามารถ', 'ไม่ได้อ่ะ', 'ไม่สามารถ', 'ไม่อ่ะ', 'ไม่ได้', 'ไม่ได้', 'ไม่ใช่', 'ไม่ได้', 'ทําไม่ได้', 'ไม่กล้าไม่กล้า', 'ไม่ได้', 'ไม่ได้', 'ไม่อ่ะ', 'ไม่อ่ะ', 'ยังไม่ได้', 'ไม่ใช่', 'ไม่เหมือนกัน', 'อย่านะ', 'ไม่ได้', 'ยังเลย', 'ยังเลย', 'ไม่ใช่', 'ไม่ได้', 'อย่าเลย', 'ไม่จําเป็น', 'ไม่เคย', 'ไม่มี', 'ไม่อ่ะ', 'หรือ', 'ไม่ใช่', 'ไม่มีอะไร', 'ไม่มีที่ไหนเลย', 'ไม่น่าจะ', 'ไม่ใช่', 'ไม่', 'ไม่ควรจะ', 'ชันไม่', 'ไม่ควร', 'ไม่ใช่', 'ไม่ได้', 'โดยไม่ต้อง', 'ไม่อ่ะ', 'ไม่อยากหรอก', 'จะไม่', 'จะไม่', 'ไม่ค่อย', 'ไม่ค่อยได้', 'แม้จะ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOOSTER_DICT_modified = {en2th.translate(k): v for k, v in BOOSTER_DICT.items()}\n",
        "print(BOOSTER_DICT_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1p2EdgGEGhs",
        "outputId": "eef7e97d-235c-4f03-b3fc-63a4ebd2eeef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'แน่นอนครับ': 0.293, 'น่าอัศจรรย์': 0.293, 'น่ากลัวจัง': 0.293, 'สมบูรณ์': 0.293, 'มาก': 0.293, 'เด็ด': 0.293, 'ลึก': 0.293, 'ไหล...': 0.293, 'ใหญ่มาก': 0.293, 'อย่างมหาศาล': 0.293, 'ทั้งหมด': 0.293, 'โดยเฉพาะอย่างยิ่ง': 0.293, 'พิเศษ': 0.293, 'พิเศษสุด': 0.293, 'สุดขั้ว': 0.293, 'สุด ๆ': 0.293, 'สวยจริงๆ': 0.293, 'พลิกภาพ': 0.293, 'flippin': 0.293, 'Name=สมุดที่อยู่ - KName': -0.293, 'การไล่ล่า': 0.293, 'ไอ้งั่ง': 0.293, 'ไอ้บ้าเอ๊ย': 0.293, 'ไอ้บ้าเอ้ย': 0.293, 'เต็มที่': 0.293, 'ร่วมเพศ': 0.293, 'อย่างมาก': 0.293, 'สวัสดี': 0.293, 'อย่างสูง': 0.293, 'เหลือเชื่อ': 0.293, 'ไม่น่าเชื่อเลย': 0.293, 'หลัก@ item: inlistbox': 0.293, 'เฉดเดียวกัน': 0.293, 'มากขึ้น': 0.293, 'มากที่สุด': 0.293, 'ล้วน ๆ': 0.293, 'ค่อนข้าง': -0.293, 'จริงๆเลย': 0.293, 'น่าทึ่งมาก': 0.293, 'ดังนั้น': 0.293, 'อย่างทั่วถึง': 0.293, 'รวมทั้งหมด': 0.293, 'โดยสิ้นเชิง': 0.293, 'อูเบอร์ค่ะ': 0.293, 'ผิดปกติ': 0.293, 'ที่สุด': 0.293, 'อย่างที่สุด': 0.293, 'เกือบแล้ว': -0.293, 'แทบจะไม่': -0.293, 'พอแล้ว': -0.293, 'ประมาณว่า': -0.293, 'ประมาณนั้น': -0.293, 'น้อยกว่า': -0.293, 'เล็กน้อย': -0.293, 'มุม:': -0.293, 'บิต': -0.293, 'เป็นครั้งคราว': -0.293, 'บางครั้ง': -0.293, 'บางส่วน': -0.293, 'ขาดแคลน': -0.293}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SENTIMENT_LADEN_IDIOMS_modified = {en2th.translate(k): v for k, v in SENTIMENT_LADEN_IDIOMS.items()}\n",
        "print(SENTIMENT_LADEN_IDIOMS_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kmEf9v5GZsh",
        "outputId": "fbc72650-d37e-4562-e29e-a7360bca5b19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ตัดมัสตาร์ด': 2, 'มือต่อปาก': -2, 'กลับมือ': -2, 'ระเบิดควัน': -2, 'มือบน': 1, 'หักขา': 2, 'ปรุงอาหารด้วยแก๊ส': 2, 'แบล็กค่ะ': 2, 'สีแดงจ้ะ': -2, 'บนลูกบอล': 2, 'ภายใต้สภาพอากาศ': -2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_CASES_modified = {en2th.translate(k): v for k, v in SPECIAL_CASES.items()}\n",
        "print(SPECIAL_CASES_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOtljSPOGKZq",
        "outputId": "14229a68-590c-41d1-a4d5-6e190ccf4ee6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'อึ': 3, 'ระเบิด': 3, 'ตูดไม่ดี': 1.5, 'ตัวแสบ': 1.5, 'ป้ายรถเมล์': 0.0, 'ใช่ ถูกต้อง': -2, 'จูบแห่งความตาย': -1.5, 'ตายเพื่อ': 3, 'หัวใจเต้น': 3.1, 'หัวใจสลาย': -2.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In The TXT File"
      ],
      "metadata": {
        "id": "NfjKyw50fhpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data from the txt\n",
        "def open_txt_file(txt_path):\n",
        "  file = open(txt_path, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "data = open_txt_file('/content/vaderSentiment/vaderSentiment/vader_lexicon.txt')"
      ],
      "metadata": {
        "id": "JYIQFOtpSrlt"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x8AFdtfCujOv",
        "outputId": "e5d64b07-e36f-490a-9777-a6d1d55b77ef"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$:\\t-1.5\\t0.80623\\t[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]\\n%)\\t-0.4\\t1.0198\\t[-1, 0, -1, 0, 0, -2, -1, 2,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create arrays for the data\n",
        "data_lists = data.split('\\n')\n",
        "data_list_lists = [x.split('\\t') for x in data_lists]"
      ],
      "metadata": {
        "id": "yMFGOQRKTL52"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there are more than 4 for each word\n",
        "count_more_than_four = 0\n",
        "\n",
        "for text in data_list_lists:\n",
        "  if len(text) != 4:\n",
        "    count_more_than_four += 1\n",
        "    print(\"The unequal to four index:\", data_list_lists.index(text))\n",
        "  continue\n",
        "print(\"no. of the unequal to four:\", count_more_than_four)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tmpvPk1UcfT",
        "outputId": "35920ffa-a441-4291-891f-183ec75e2387"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no. of the unequal to four: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there are more than one words\n",
        "count_blank_spaces = 0\n",
        "for text in data_list_lists:\n",
        "  if ' ' in text[0]:\n",
        "    count_blank_spaces += 1\n",
        "    print(\"The blank space index:\", data_list_lists.index(text))\n",
        "  continue\n",
        "print(\"no. of blank spaces:\", count_blank_spaces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCnAd7Y-X5be",
        "outputId": "81a5cd56-4b10-4254-e723-3bd33a8b8dc7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The blank space index: 5\n",
            "The blank space index: 1260\n",
            "The blank space index: 2865\n",
            "The blank space index: 5908\n",
            "no. of blank spaces: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blank_space_lists = [5, 1260, 2865, 5908]\n",
        "\n",
        "for idx in blank_space_lists:\n",
        "  print(data_list_lists[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orLnqFeoYNrs",
        "outputId": "5eefdb64-20f2-4cd3-ef39-cb49c8d33e01"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"( '}{' )\", '1.6', '0.66332', '[1, 2, 2, 1, 1, 2, 2, 1, 3, 1]']\n",
            "[\"can't stand\", '-2.0', '0.63246', '[-2, -2, -2, -1, -1, -2, -3, -2, -2, -3]']\n",
            "['fed up', '-1.8', '1.249', '[-2, -2, -3, -4, -2, -2, -1, -2, -1, 1]']\n",
            "['screwed up', '-1.5', '0.67082', '[-2, -2, -2, -1, -1, 0, -2, -1, -2, -2]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Method to check as the word is English.\n",
        "# import nltk\n",
        "# nltk.download('words')\n",
        "# from nltk.corpus import words\n",
        "\n",
        "# samplewords=['apple','a%32','j & quod','rectangle','house','fsdfdsoij','fdfd']\n",
        "# [i for i in samplewords if i in words.words()]"
      ],
      "metadata": {
        "id": "_F9QfFY2YqpP"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_lists = [5, 1260, 2865, 5908]\n",
        "\n",
        "for idx in index_lists:\n",
        "  sample_data_in_lists = data_list_lists[idx][0]\n",
        "  print(\"Text :\", sample_data_in_lists)\n",
        "  print(\"Is English: \", sample_data_in_lists in words.words())\n",
        "  print(\"==================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12-MYOvpaDLx",
        "outputId": "8917ae4a-fee8-4a8f-c454-a44ead23eb03"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text : ( '}{' )\n",
            "Is English:  False\n",
            "==================\n",
            "Text : can't stand\n",
            "Is English:  False\n",
            "==================\n",
            "Text : fed up\n",
            "Is English:  False\n",
            "==================\n",
            "Text : screwed up\n",
            "Is English:  False\n",
            "==================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments = []\n",
        "en_sentiment_index = []\n",
        "count_translated = 0\n",
        "count_all_texts = 0\n",
        "for text in data_list_lists:\n",
        "  count_all_texts += 1\n",
        "  #If english, translate\n",
        "  if text[0] in words.words():\n",
        "    en_sentiment_index.append(data_list_lists.index(text))\n",
        "    th_texts = en2th.translate(text[0])\n",
        "    new_th_sentiments = [th_texts, text[1], text[2], text[3]]\n",
        "    th_text_sentiments.append(new_th_sentiments)\n",
        "    count_translated += 1\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "pxFIZqIYTfYF"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete error on translation\n",
        "#Name=สมุดที่อยู่ - KName error in translation in PyThaiNLP\n",
        "#@ item: inlistbox\n",
        "th_sentiment_index_delelted = []\n",
        "for arr in th_text_sentiments:\n",
        "  if 'Name=สมุดที่อยู่' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "\n",
        "  elif '@ item: inlistbox' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "\n",
        "  elif 'Name=สมุดที่อยู่ - KName' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)  \n",
        "\n",
        "  elif '@ info: whatsthis' in arr[0]:\n",
        "    th_sentiment_index_delelted.append(th_text_sentiments.index(arr))\n",
        "    th_text_sentiments.remove(arr)\n",
        "  else:\n",
        "    continue\n",
        "#9558"
      ],
      "metadata": {
        "id": "EGQintjNvnsM"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_list_lists), count_all_texts, len(th_text_sentiments), count_translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_SpobA7oCB7",
        "outputId": "63115382-352d-47b3-ec70-30d0fc10dd29"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7520 7520 2651 3997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA_IV6oMoFM2",
        "outputId": "f6434679-52e2-4d43-bd77-b2c37a5a4fad"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ฟู', '-3.7', '0.45826', '[-3, -4, -4, -3, -3, -4, -4, -4, -4, -4]'],\n",
              " ['เทฮัก', '1.9', '0.7', '[3, 1, 2, 2, 1, 2, 3, 2, 1, 2]'],\n",
              " ['มือ', '2.2', '0.87178', '[2, 2, 1, 3, 2, 3, 4, 1, 2, 2]'],\n",
              " ['หัวใจ', '3.2', '0.63246', '[3, 3, 4, 3, 4, 2, 3, 4, 3, 3]'],\n",
              " ['หัวใจ', '3.3', '0.48305', '[4, 3, 3, 3, 4, 3, 3, 3, 4, 3]']]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_en_sentiment = index[:5]\n",
        "for idx in check_en_sentiment:\n",
        "  print(data_list_lists[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6dCvviKr5bS",
        "outputId": "bb93bd74-0885-44cc-d9eb-4bd6c440e268"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(-:{', '-0.1', '1.57797', '[-2, -3, 1, -2, 1, 1, 0, 0, 2, 1]']\n",
            "['(;<', '0.3', '1.00499', '[1, 2, -1, -1, 0, 0, 1, -1, 1, 1]']\n",
            "['(^;', '1.5', '0.5', '[1, 2, 2, 1, 2, 1, 2, 1, 1, 2]']\n",
            "['(^;o', '1.9', '0.83066', '[2, 2, 1, 2, 1, 4, 2, 1, 2, 2]']\n",
            "['*-;', '2.4', '1.62481', '[2, 3, 4, 4, 2, 1, -1, 4, 1, 4]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Change Arrays into String"
      ],
      "metadata": {
        "id": "9gcjNMssuN-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_text_sentiments[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGWac3R2uyv1",
        "outputId": "74fe4432-ae99-4894-a083-7ac9ff8841df"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ฟู', '-3.7', '0.45826', '[-3, -4, -4, -3, -3, -4, -4, -4, -4, -4]']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\\t for one blank space, \\n for the new spaces\n",
        "join_with_blank_spaces = ['\\t'.join(text) for text in th_text_sentiments]\n",
        "final_th_text_with_sentiment = '\\n'.join(join_with_blank_spaces)"
      ],
      "metadata": {
        "id": "oOceqa3iudbN"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_th_text_with_sentiment[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HCVH5trEvPrg",
        "outputId": "c4065e53-2a04-4916-da2f-1f6b23755e51"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ฟู\\t-3.7\\t0.45826\\t[-3, -4, -4, -3, -3, -4, -4, -4, -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add translated to the txt file"
      ],
      "metadata": {
        "id": "VCizWC0Prw8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspiration: https://colab.research.google.com/github/computationalcore/introduction-to-python/blob/master/notebooks/4-files/PY0101EN-4-2-WriteFile.ipynb#scrollTo=Ar3kQmbt0uq3\n",
        "\n",
        "lexicon_path = '/content/vaderSentiment/vaderSentiment/vader_lexicon.txt'\n",
        "with open(lexicon_path, 'a') as new_lexicon:\n",
        "  new_lexicon.write(final_th_text_with_sentiment)"
      ],
      "metadata": {
        "id": "MJw8N9v6rziY"
      },
      "execution_count": 115,
      "outputs": []
    }
  ]
}