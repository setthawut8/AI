{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding Layer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP37akhETaG4TdD3yp/kcAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setthawut8/ai/blob/main/%5BEmbedding%20Layer%5D%20tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embedding Layer\n",
        "(Inspiration https://youtu.be/Fuw0wv3X-0o)"
      ],
      "metadata": {
        "id": "G3n-OBJyhe0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4HpNOKaCYGYg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = ['nice food',\n",
        "           'amazing restaurant',\n",
        "           'too good',\n",
        "           'just loved it',\n",
        "           'will go again',\n",
        "           'horrible food',\n",
        "           'never go there',\n",
        "           'poor service',\n",
        "           'poor quality',\n",
        "           'needs improvement']\n",
        "\n",
        "sentiments = np.array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "qbSDgReBYtNI"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) One Hot Encoding"
      ],
      "metadata": {
        "id": "y70XYKUpcoPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create an array of number of each word\n",
        "#(encoded number, size of vocab)\n",
        "one_hot(\"amazing restaurant\", 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWO8hZ9nZJAR",
        "outputId": "da794db2-7945-4280-a2b7-0ca25de31327"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no. of vocabulary words to be kept\n",
        "vocab_size=50\n",
        "#encoding words\n",
        "encoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n",
        "encoded_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neAYPM1kZPdt",
        "outputId": "083dd710-6d8f-4906-bd2f-14922710c307"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[22, 19],\n",
              " [38, 42],\n",
              " [4, 31],\n",
              " [41, 27, 9],\n",
              " [19, 37, 31],\n",
              " [40, 19],\n",
              " [35, 37, 32],\n",
              " [7, 44],\n",
              " [7, 1],\n",
              " [27, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "itK80oY8fulQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Padding"
      ],
      "metadata": {
        "id": "oZ79eCP9ctV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#some pharses are 2 or 3 words so some needed to padding to have the same length of array\n",
        "maxlen=3\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=maxlen, padding='post')\n",
        "padded_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPTdq77DbRhT",
        "outputId": "de21fa55-67af-4923-8e4a-d2f7a4b49711"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[22, 19,  0],\n",
              "       [38, 42,  0],\n",
              "       [ 4, 31,  0],\n",
              "       [41, 27,  9],\n",
              "       [19, 37, 31],\n",
              "       [40, 19,  0],\n",
              "       [35, 37, 32],\n",
              "       [ 7, 44,  0],\n",
              "       [ 7,  1,  0],\n",
              "       [27,  6,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Embedded Vector Size (features of each word) & Model"
      ],
      "metadata": {
        "id": "vbPmI3fTc-JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embeded vector size = numbers of features in a word\n",
        "embeded_vector_size = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embeded_vector_size, input_length=maxlen, name='embedding'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "hUsj08q6dC5H"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_reviews\n",
        "y = sentiments"
      ],
      "metadata": {
        "id": "0JSZT-5veUfA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMCcHldiea9-",
        "outputId": "f24eed0b-61e9-4907-81c8-2767bf8c62ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 4)              200       \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWHag2Fgehzh",
        "outputId": "72c831f7-b61f-43d1-993b-879936860176"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4265cac910>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X, y)\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_MNuASwfCxC",
        "outputId": "7541a9d8-a200-4814-d367-cf83be1e6646"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6026 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "len(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnGcvkjpfJIL",
        "outputId": "da7621fe-73f3-435d-cacc-5b7e81d2052e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights[22])\n",
        "print(weights[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0ssz7jJfRUL",
        "outputId": "4449eb80-50d2-4f09-8dc6-a1e2e18f16d6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.09223264 -0.00312139 -0.01635139  0.01045673]\n",
            "[ 0.01558313 -0.01015184 -0.04265321  0.01204729]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#follow this link for downloading weights for the embedding layer\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/#:~:text=4.%20Example%20of%20Using%20Pre%2DTrained%20GloVe%20Embedding"
      ],
      "metadata": {
        "id": "73iBxJ8af4-I"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec \n",
        "(https://youtu.be/hQwFeIupNP0)"
      ],
      "metadata": {
        "id": "BFy0NS5lhyOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JXVKosahhzbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}